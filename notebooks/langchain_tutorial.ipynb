{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2901fffc-eeed-4019-93b5-c90fe1430816",
   "metadata": {},
   "source": [
    "# LangChain #\n",
    "<ul>\n",
    "    <ul>\n",
    "        <li>LangChain is a framework the helps build applications around LLM's</li>\n",
    "        <li>Its like a tool box for connecting LLM's with memory, API's, chains and agents</li>\n",
    "        <li>The Fundamental bricks of Framework include LLM, PromptTemplate, Chains, Memory and Agents</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c14b0-e6b9-477d-9911-0175131439cc",
   "metadata": {},
   "source": [
    "## Brick 1---> <u>LLM</u> ##\n",
    "\n",
    "<ul>\n",
    "    <li>In LangChain an LLM Model is component that one can plug-in and swap</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca76cb6d-00d4-4d70-9d90-e9066d07880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangGraph, unlike LangChain, focuses on providing a graph-based representation of language data, enabling more efficient and effective handling of complex relationships and patterns within the data, which can be particularly useful for tasks such as question answering, information extraction, and text summarization.\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Simple LLM Call\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "\n",
    "# Connecting to local llm mistral\n",
    "llm = Ollama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.2 # [0.3--> More deterministic and similar responses < 1.1 More Creative ]\n",
    "    # For Factual and QA Based answering we want system to more reliable\n",
    "    # So, in this context it is important to keep temperature low\n",
    ")\n",
    "\n",
    "\n",
    "# LLM Call\n",
    "response = llm(\"Explain the need of LangGraph in one sentence when we already have LangChain\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed89bb4-8d62-4035-a4db-358c7342fb08",
   "metadata": {},
   "source": [
    "## Brick 2: PromptTemplate ##\n",
    "\n",
    "<ul>\n",
    "    <li>It is a reusable blue print of instruction we can give LLM's with {placeholders}</li>\n",
    "    <li>Helps writing clean and dynamic content</li>\n",
    "    <li>It is helpful when structure and task is same but content changes everytime</li>\n",
    "</ul>\n",
    "<p3>When to be careful</p3>\n",
    "<ol>\n",
    "    <li>If the task itself varies each time [One time Poem, another time SQL Query]</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86616163-40c9-420b-87cd-3d06178b9978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artificial General Intelligence (AGI), also known as \"strong AI,\" refers to a type of artificial intelligence that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks at a level equal to or beyond human capability. Unlike Narrow AI, which is designed to perform specific tasks, AGI can reason, problem-solve, and think abstractly like humans.\n",
      "\n",
      "The purpose of AGI is to create a machine that can perform any intellectual task that a human can do. This includes understanding complex concepts, learning from experience, making decisions under uncertainty, and even exhibiting creativity and consciousness. The ultimate goal is to build an AI system that can match or surpass human intelligence in its versatility and depth.\n",
      "\n",
      "Here are three fundamental use cases of AGI:\n",
      "\n",
      "1. Healthcare: AGI could revolutionize healthcare by analyzing vast amounts of medical data, diagnosing diseases more accurately than humans, and suggesting personalized treatment plans based on a patient's unique genetic makeup and lifestyle factors. It could also assist in drug discovery by predicting the effects of new compounds and optimizing their design.\n",
      "\n",
      "2. Education: AGI can adapt to individual learning styles and abilities, providing personalized instruction tailored to each student's needs. It can help students master complex concepts by explaining them in multiple ways until they are fully understood. AGI-powered tutors could also be available 24/7, allowing students to learn at their own pace and convenience.\n",
      "\n",
      "3. Economics: AGI can analyze market trends, predict consumer behavior, and optimize business strategies based on real-time data. It can help companies make informed decisions about pricing, marketing, and resource allocation, ultimately leading to increased efficiency and profitability. In finance, AGI could also be used for risk assessment, fraud detection, and algorithmic trading.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# We have to ensure that task remains same everytime before writing template\n",
    "\n",
    "template = \"\"\"\n",
    "Explain the defination and purpose about the {concept} and give 3 fundamental usecase\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "# Fill here the actual concept\n",
    "final_prompt = prompt.format(concept=\"Artificial General Intellegence\")\n",
    "\n",
    "response = llm(final_prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4e9eb-1d59-4697-a525-04f5ad4cfcd2",
   "metadata": {},
   "source": [
    "## Brick 3: Chains ## \n",
    "\n",
    "<ul>\n",
    "    <li>A Chain is a sequence of components[LLM's, Prompts, Tools, Memory etc..] linked together into one reusable pipeline</li>\n",
    "    <li>Facilitates Flexibility and Reusability of Code</li>\n",
    "    <li>We can define multiple components and use various combinations without writing the code of each combination every time seperatly</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ad5b98-9db0-4b22-a7f1-8aeb782121f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artificial General Intelligence (AGI) refers to a type of AI that has the ability to understand, learn, and apply knowledge across a wide range of tasks at a level equal to or beyond human capacity.\n",
      "<class 'str'>\n",
      "{'topic': 'AGI', 'text': ' Artificial General Intelligence (AGI) refers to a type of AI that has the ability to understand, learn, and apply knowledge across a wide range of tasks at a level equal to or beyond human capacity.'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Reusable Template\n",
    "template = \"Explain about the {topic} in one sentence\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_varibales=[\"topic\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Stacking of Multiple Components using Chains\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "response = chain.run(\"AGI\")\n",
    "response2 = chain({\"topic\": \"AGI\"})\n",
    "\n",
    "print(response)\n",
    "print(type(response))\n",
    "\n",
    "print(response2)\n",
    "print(type(response2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbda75-05f7-41e7-82b3-00c38d27bfb7",
   "metadata": {},
   "source": [
    "<h4> Stacking of Multiple Chains </h4>\n",
    "<ul>\n",
    "    <li>Define Multiple Components and Chains and use different combinations</li>\n",
    "    <li>Helps achieve code reusability</li>\n",
    "</ul>\n",
    "<p4>Below, is an example of 3 chains [Summarize---> Translate ----> Simplify]</p4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e386f32-b765-4a19-b718-c406d1378e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' Precision is one of the most important metric in machine learning. It measures the proportion of positive predictions that are\\nactually correct.It gives us an idea how confident the model is when it comes it predictions\\n', 'summary': \" Precision in machine learning is a significant metric, indicating the ratio of correctly predicted positive instances to the total predicted positives. This measure helps gauge the confidence level of a model's predictions regarding positive outcomes.\", 'translation': \" La précision dans l'apprentissage automatique est un indicateur important, indiquant le rapport entre les instances positives correctement prédites et le total des instances positives prévues. Cette mesure permet d'évaluer le niveau de confiance du modèle quant aux résultats positifs prévus.\\n\\nEn français : La précision dans l'apprentissage automatique est un indicateur important, indiquant le rapport entre les instances positives correctement prédites et le total des instances positives prévues. Cette mesure permet d'évaluer le niveau de confiance du modèle quant aux résultats positifs prévus.\", 'simple_french': \" En français simple pour un enfant de 10 ans :\\n\\nLa précision dans l'apprentissage automatique est comme une étoile qui montre comment bien notre ordinateur peut prévoir les bonnes réponses. Cela se fait en regardant le nombre de fois où il a prédit correctement par rapport au nombre total de fois où il devait prévoir des bonnes réponses. Cela nous permet de savoir combien on peut confier aux résultats positifs que notre modèle donne.\\n\\nEn français : La précision dans l'apprentissage automatique est une étoile qui montre comment bien notre ordinateur peut prévoir les bonnes réponses. Cela se fait en regardant le nombre de fois où il a prédit correctement par rapport au nombre total de fois où il devait prévoir des bonnes réponses. Cela nous permet de savoir combien on peut confier aux résultats positifs que notre modèle donne.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Step1: Summarize\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Summarize the following in 2 Sentences \\n\\n: {text}\"\n",
    ")\n",
    "\n",
    "summarize_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=summary_prompt,\n",
    "    output_key=\"summary\" \n",
    "    # Each LLMChain produces output in the form of dictionary\n",
    "    # \"output_key\" decides the dictionary key for that chain’s output so you can easily access it or pass it to the next chain.\n",
    ")\n",
    "\n",
    "# Step2: Translate to French Language\n",
    "translate_prompt = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=\"Translate the following into French \\n\\n: {summary}\",\n",
    ")\n",
    "translate_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=translate_prompt,\n",
    "    output_key=\"translation\"\n",
    ")\n",
    "\n",
    "# Step3: Simplify in simple words\n",
    "simplify_prompt = PromptTemplate(\n",
    "    input_variables=[\"translation\"],\n",
    "    template=\"Explain this in simple French like for a 10 year old:\\n\\n{translation}\"\n",
    ")\n",
    "simplify_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=simplify_prompt,\n",
    "    output_key=\"simple_french\"\n",
    ")\n",
    "\n",
    "# Combine Into SequentialChain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[summarize_chain, translate_chain, simplify_chain],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"summary\", \"translation\", \"simple_french\"]\n",
    "    # output_variables in the SequentialChain tells the overall chain \n",
    "    # which keys you want to keep in the final result\n",
    ")\n",
    "\n",
    "text =  \"\"\" Precision is one of the most important metric in machine learning. It measures the proportion of positive predictions that are\n",
    "actually correct.It gives us an idea how confident the model is when it comes it predictions\n",
    "\"\"\"\n",
    "\n",
    "result = overall_chain({\"text\": text})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a78438-d4a4-48dd-9314-c73a66b3191e",
   "metadata": {},
   "source": [
    "## Brick 4: Memory ##\n",
    "\n",
    "<ul>\n",
    "    <li>Memory allows the chain or agent to remember information across multiple calls</li>\n",
    "    <li>Without memory every call is stateless, the model forgets everything after one prompt</li>\n",
    "</ul>\n",
    "\n",
    "<b>Types</b>\n",
    "<ol>\n",
    "    <li>ConversationalBufferMemory --> Keeps a running string of past conversations</li>\n",
    "    <li>ConversationalSummarymemory --> Keeps a summarized content to save space</li>\n",
    "    <li>Vector/Document memory ---> Stores embeddings for retrival [Used in RAG's]</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0d347-a96c-4766-b42d-5421ee389a82",
   "metadata": {},
   "source": [
    "<b>ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb3f2b8d-9c4e-47c7-8a3c-7b4de9d49957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The per capita income of India in the year 2001 was approximately $430 (USD). This value is subject to slight variations depending on the source, but this figure provides a general idea. For more accurate and up-to-date information, I recommend checking official sources such as the World Bank or the Indian government's statistical department.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Initialize the memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", # Key to store the Chat\n",
    "    return_messages = True # returns converstion history to LLM\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\", \"chat_history\"],\n",
    "    template=\"\"\" Act as an helpful assistannt and take {chat_history} as context\n",
    "    and give answers to the {user_input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "overall_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "user_input = \"What was the Percapita Income of India in 2001?\"\n",
    "response1 = overall_chain.run(user_input)\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1018d427-716c-4625-a6d8-503dbaed4de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, I can translate that in French. The per capita income of India in the year 2001 was approximately 430 dollars (USD). This value is subject to slight variations depending on the source, but this figure provides a general idea. For more accurate and up-to-date information, I recommend checking official sources such as the World Bank or the Indian government's statistical department.\n",
      "\n",
      "In French: Oui, je peux traduire ça en français. Le revenu par habitant de l'Inde en l'an 2001 était approximativement 430 dollars (USD). Cette valeur est sujette à des variations légères selon la source, mais cette chiffre fournit une idée générale. Pour des informations plus précises et à jour, je recommande de vérifier des sources officielles telles que la Banque mondiale ou le département statistique indien du gouvernement.\n"
     ]
    }
   ],
   "source": [
    "response2 = overall_chain.run(user_input=\"Can you translate that in French?\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b3e0271-927f-4a82-8156-e7d4af657224",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = overall_chain.run(\n",
    "    user_input=\"Give Per Capita Income in Rupees\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098f50e-95e7-428c-b331-ade4528faeff",
   "metadata": {},
   "source": [
    "<b>ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd29ef45-31f9-4bb7-9646-973483d319cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The per capita income of India in the year 2001 was approximately $430 (USD) according to World Bank data. However, it's important to note that inflation and exchange rates can affect these figures over time, so for the most accurate and up-to-date information, I recommend visiting a reliable source such as the World Bank or the Indian government's official statistics portal.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Initialize the memory\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,  # summary memory needs an llm to generate summaries\n",
    "    memory_key=\"chat_history\", # Key to store the Chat\n",
    "    return_messages = True # returns converstion history to LLM\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\", \"chat_history\"],\n",
    "    template=\"\"\" Act as an helpful assistannt and take {chat_history} as context\n",
    "    and give answers to the {user_input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "overall_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "user_input = \"What was the Percapita Income of India in 2001?\"\n",
    "response1 = overall_chain.run(user_input)\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d30d458-d3d3-45fb-9297-c276c3289378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': 'How much did it increase in 2011?', 'chat_history': [SystemMessage(content=\" The human asks about the Per Capita Income of India in 2001. The AI responds that it was approximately $430 (USD) according to World Bank data, but notes that inflation and exchange rates can affect these figures over time, suggesting visiting a reliable source like the World Bank or Indian government's official statistics portal for accurate and up-to-date information.\", additional_kwargs={}, response_metadata={})], 'text': \" According to World Bank data, the per capita income of India increased significantly from approximately $430 (USD) in 2001 to around $1,057 (USD) in 2011. However, it's important to note that inflation and exchange rates can affect these figures over time. For accurate and up-to-date information, I recommend visiting a reliable source like the World Bank or Indian government's official statistics portal.\"}\n"
     ]
    }
   ],
   "source": [
    "response2 = overall_chain({\"user_input\": \"How much did it increase in 2011?\"})\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a7240e-63c9-407d-95f0-f5061009670d",
   "metadata": {},
   "source": [
    "<h4><b></b>VectorStoreRetreivereMemory [RAG]</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9896ad-4210-4773-a58a-4006376093cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "\n",
    "def extract_chunks_from_page(text, filename, page_num, chunk_size=300, overlap=50):\n",
    "    \"\"\"\n",
    "    PdfPlumber extracts the text of a one page at a time.\n",
    "    To extract the text of all the pages we have to run a loop, where in each iteration the text of single page will be removed.\n",
    "    After removal of text this function will be called, which takes the text as the input of a single page and creates chunks.\n",
    "    {A Chunk is a fixed sized segment of a text document from a consective text split}\n",
    "    \"\"\"\n",
    "    # The text is split into a list of words to tokenize\n",
    "    words = text.strip().split()\n",
    "    chunks=[]\n",
    "    # Here chunks will be created iteratively\n",
    "    # Each chunk will have 300 words with a 50 word overlap\n",
    "    for word in range(0, len(words), chunk_size-overlap):\n",
    "        chunk_words = words[word:word+chunk_size]\n",
    "        if len(chunk_words)<30: # Skip if the length of the chunk words is less than 30\n",
    "            continue\n",
    "        chunk_text = \" \".join(chunk_words) # This helps combining all the list of words into a single string\n",
    "        chunk_id = f\"{filename}_p{page_num}_c{word}\"\n",
    "        # Will create a dictionary that returns meta data and chunked text of each page\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"page\": page_num,\n",
    "                \"Retrieval_ID\": chunk_id,\n",
    "                \"text\": chunk_text\n",
    "            }\n",
    "        ) # This will return a list of dictionaries\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f275182-98a6-4683-af71-d5fb76618138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_from_pdf(DATA_DIR):\n",
    "    # Accessing all the filenames from the directory\n",
    "    all_chunks=[]\n",
    "    pdf_files = [filename for filename in os.listdir(DATA_DIR) if filename.endswith(\".pdf\")]\n",
    "    for filename in pdf_files:\n",
    "        pdf_path = os.path.join(DATA_DIR, filename)\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                text = page.extract_text()\n",
    "                if not text:\n",
    "                    continue\n",
    "                text = text.replace(\"/n\", \"\").strip()\n",
    "                chunks = extract_chunks_from_page(text, filename, page_num)\n",
    "                all_chunks.extend(chunks)\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chunks = extract_chunks_from_pdf(DATA_DIR)\n",
    "    df = pd.DataFrame(chunks)\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0add5d7-ecfc-48fa-844b-1597610d4455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>page</th>\n",
       "      <th>Retrieval_ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p1_c0</td>\n",
       "      <td>CIN: L65920MH1994PLC080618 HDFC Bank Limited, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p1_c250</td>\n",
       "      <td>by way of remote e-voting will not be able to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p2_c0</td>\n",
       "      <td>resolutions as read. Now, without any further ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p2_c250</td>\n",
       "      <td>present. With permission of the members, I cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p3_c0</td>\n",
       "      <td>Friends, we would like to recall last year whe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  page  \\\n",
       "0  HDFC_AGM_Transcript_Aug2024.pdf     1   \n",
       "1  HDFC_AGM_Transcript_Aug2024.pdf     1   \n",
       "2  HDFC_AGM_Transcript_Aug2024.pdf     2   \n",
       "3  HDFC_AGM_Transcript_Aug2024.pdf     2   \n",
       "4  HDFC_AGM_Transcript_Aug2024.pdf     3   \n",
       "\n",
       "                              Retrieval_ID  \\\n",
       "0    HDFC_AGM_Transcript_Aug2024.pdf_p1_c0   \n",
       "1  HDFC_AGM_Transcript_Aug2024.pdf_p1_c250   \n",
       "2    HDFC_AGM_Transcript_Aug2024.pdf_p2_c0   \n",
       "3  HDFC_AGM_Transcript_Aug2024.pdf_p2_c250   \n",
       "4    HDFC_AGM_Transcript_Aug2024.pdf_p3_c0   \n",
       "\n",
       "                                                text  \n",
       "0  CIN: L65920MH1994PLC080618 HDFC Bank Limited, ...  \n",
       "1  by way of remote e-voting will not be able to ...  \n",
       "2  resolutions as read. Now, without any further ...  \n",
       "3  present. With permission of the members, I cal...  \n",
       "4  Friends, we would like to recall last year whe...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adafe827-fe79-4a6e-a3df-3cee36c7a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Loading the LLM Model {Brick1}\n",
    "llm = Ollama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.3)\n",
    "\n",
    "\n",
    "# Creating a Template {Brick2}\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"chat_history\", \"context\", \"user_input\"],\n",
    "    template = \"\"\"\n",
    "    Act as a Financial QA Assistant and provide structural insights using the \n",
    "    {context} and the {chat_history} to the {user_input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Initializing the Memory {Brick4}\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    input_key=\"user_input\"\n",
    ")\n",
    "\n",
    "# Chaining all the components\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# A Document is a tiny container LangChain uses to hold a chunk of text plus optional metadata.\n",
    "documents = [\n",
    "    Document(page_content=row[\"text\"], metadata={\"filename\":row[\"filename\"],\n",
    "                                                 \"page\":row[\"page\"],\n",
    "                                                 \"Retrieval_ID\":row[\"Retrieval_ID\"]})\n",
    "    for index, row in df.iterrows()\n",
    "]\n",
    "\n",
    "embedding_transformer = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_storage = Chroma.from_documents(documents, embedding_transformer)\n",
    "retriever = vector_storage.as_retriever(search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f542cfc-5906-42bf-9c09-4fc4adc54126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The document appears to be a transcript from a meeting or presentation, possibly related to the annual report of HDFC Bank. The speaker, Mr. Atanu Chakraborty, is discussing the challenges faced by banking systems and corporates regarding mobile numbers, and how their company is addressing these issues with their best efforts. He also mentions that detailed information about specific items such as audit fees, legal and professional fees can be found in the Annual Report, which consists of over 500 pages. The speaker expresses gratitude for the patience of the audience during the presentation and thanks them for their attention. However, the document does not provide a brief summary of the document itself.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparaion of input_variables [\"user_input\", \"context\", \"chat_history\"]\n",
    "\n",
    "user_input = \"Explain the brief summary of the document\"\n",
    "chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "docs = retriever.get_relevant_documents(user_input)\n",
    "context = \"/n\".join(\n",
    "    [x.page_content for x in docs]\n",
    ")\n",
    "chain.run(user_input=user_input, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "940ea668-1e36-423f-b02b-3581c6203250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Le document en question est une transcription d'une réunion ou présentation liée à l'annuel de HDFC Bank, où M. Atanu Chakraborty discute des défis liés aux numéros de mobile rencontrés par les systèmes bancaires et les entreprises, ainsi que comment HDFC Bank y travaille pour résoudre ces problèmes. Il mentionne également que les informations détaillées sur certains éléments spécifiques comme les frais d'audit et les frais de droit et professionnels peuvent être trouvés dans différentes pages du rapport annuel, qui compte plus de 500 pages. Le locuteur remercie l'audience pour leur patience et attention, mais le document ne fournit pas une résumé court de lui-même.\\n\\n(Traduction automatique)\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input2 = \"Can you translate that in french?\"\n",
    "chain.run(user_input=user_input2, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca940e6a-5fe4-4abc-8524-00249875e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, DATA_DIR, chunk_size=300, overlap=50, min_chunk_len=30):\n",
    "        self.data_dir = DATA_DIR\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.min_chunk_len = min_chunk_len\n",
    "\n",
    "    def _extract_chunks_from_page(self, text, filename, page_num):\n",
    "        words = text.strip().split()\n",
    "        chunks = []\n",
    "        for word in range(0, len(words), self.chunk_size - self.overlap):\n",
    "            chunk_words = words[word:word + self.chunk_size]\n",
    "            if len(chunk_words) < self.min_chunk_len:\n",
    "                continue\n",
    "            chunk_text = \" \".join(chunk_words)\n",
    "            chunk_id = f\"{filename}_p{page_num}_c{word}\"\n",
    "            chunks.append(\n",
    "                {\n",
    "                    \"filename\": filename,\n",
    "                    \"page\": page_num,\n",
    "                    \"Retrieval_ID\": chunk_id,\n",
    "                    \"text\": chunk_text\n",
    "                }\n",
    "            )\n",
    "        return chunks\n",
    "\n",
    "    def extract_from_pdfs(self):\n",
    "        all_chunks = []\n",
    "        pdf_files = [f for f in os.listdir(self.data_dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "        for filename in pdf_files:\n",
    "            pdf_path = os.path.join(self.data_dir, filename)\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                    text = page.extract_text()\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    text = text.replace(\"\\n\", \" \").strip()\n",
    "                    chunks = self._extract_chunks_from_page(text, filename, page_num)\n",
    "                    all_chunks.extend(chunks)\n",
    "\n",
    "        return pd.DataFrame(all_chunks)\n",
    "\n",
    "\n",
    "class VectorStoreManager:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.embedding_transformer = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        self.vector_storage = None\n",
    "\n",
    "    def build_store(self, df):\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=row[\"text\"],\n",
    "                metadata={\n",
    "                    \"filename\": row[\"filename\"],\n",
    "                    \"page\": row[\"page\"],\n",
    "                    \"Retrieval_ID\": row[\"Retrieval_ID\"]\n",
    "                }\n",
    "            )\n",
    "            for index, row in df.iterrows()\n",
    "        ]\n",
    "        self.vector_storage = Chroma.from_documents(documents, self.embedding_transformer)\n",
    "        return self.vector_storage\n",
    "\n",
    "    def get_retriever(self, k=3):\n",
    "        if not self.vector_storage:\n",
    "            raise ValueError(\"Vector storage has not been initialized. Call build_store() first.\")\n",
    "        return self.vector_storage.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "\n",
    "class FinancialQA_Assistant:\n",
    "    def __init__(self, model=\"mistral\", temperature=0.3):\n",
    "        self.llm = Ollama(model=model, temperature=temperature)\n",
    "        self.memory = ConversationSummaryMemory(\n",
    "            llm=self.llm,\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            input_key=\"user_input\"\n",
    "        )\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \"context\", \"user_input\"],\n",
    "            template=\"\"\"\n",
    "            Act as a Financial QA Assistant and provide structural insights using \n",
    "            the {context} and the {chat_history} to answer: {user_input}\n",
    "            \"\"\"\n",
    "        )\n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt, memory=self.memory)\n",
    "\n",
    "    def run(self, user_input, retriever):\n",
    "        chat_history = self.memory.load_memory_variables({})[\"chat_history\"]\n",
    "        docs = retriever.get_relevant_documents(user_input)\n",
    "        context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        return self.chain.run(user_input=user_input, context=context)\n",
    "\n",
    "\n",
    "class FinancialQAPipeline:\n",
    "    def __init__(self, data_dir):\n",
    "        self.processor = DocumentProcessor(data_dir)\n",
    "        self.vector_manager = VectorStoreManager()\n",
    "        self.assistant = FinancialQA_Assistant()\n",
    "\n",
    "    def run(self, user_query):\n",
    "        df = self.processor.extract_from_pdfs()\n",
    "        self.vector_manager.build_store(df)\n",
    "        retriever = self.vector_manager.get_retriever(k=3)\n",
    "        return self.assistant.run(user_query, retriever)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = FinancialQAPipeline(DATA_DIR)\n",
    "    answer = pipeline.run(\"Explain the brief summary of the document\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341cc338-b40b-45b7-8980-4f3c9a7b068d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
